@inproceedings{parida-etal-2023-havqa,
    title = "{H}a{VQA}: A Dataset for Visual Question Answering and Multimodal Research in {H}ausa Language",
    author = "Parida, Shantipriya  and
      Abdulmumin, Idris  and
      Muhammad, Shamsuddeen Hassan  and
      Bose, Aneesh  and
      Kohli, Guneet Singh  and
      Ahmad, Ibrahim Said  and
      Kotwal, Ketan  and
      Deb Sarkar, Sayan  and
      Bojar, Ond{\v{r}}ej  and
      Kakudi, Habeebah",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.646",
    doi = "10.18653/v1/2023.findings-acl.646",
    pages = "10162--10183",
    abstract = "This paper presents {``}HaVQA{''}, the first multimodal dataset for visual question answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, the dataset provides 12,044 gold standard English-Hausa parallel sentences that were translated in a fashion that guarantees their semantic match with the corresponding visual information. We conducted several baseline experiments on the dataset, including visual question answering, visual question elicitation, text-only and multimodal machine translation.",
}
